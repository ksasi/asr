{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215d0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/asr/speechbrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9223dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from taylor_series_linear_attention import TaylorSeriesLinearAttn\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from inspect import signature \n",
    "\n",
    "import speechbrain as sb\n",
    "from speechbrain.utils.distributed import if_main_process, run_on_main\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574daee",
   "metadata": {},
   "source": [
    "### Pretrained Conformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe40ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_conformer = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a40e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LinearAttention a wrapper for Taylor Series LinearAttention\n",
    "class LinearAttn(torch.nn.Module):\n",
    "    \"\"\"Wrapper Class for Taylor Series LinearAttention\"\"\"\n",
    "    def __init__(self, dim = 512, dim_head = 16, heads = 8):\n",
    "        super(LinearAttn, self).__init__()\n",
    "        self.attn = TaylorSeriesLinearAttn(dim = dim, dim_head = dim_head, heads = heads)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask, key_padding_mask, pos_embs):\n",
    "        # Ignoring key_padding_mask and pos_embs for TaylorSeriesLinearAttn\n",
    "        out = self.attn(query, mask=attn_mask)\n",
    "        return out, self.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"/home/ubuntu/asr/datasets/LibriSpeech/test-clean/61/70970/61-70970-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cf5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training procedure\n",
    "class ASR(sb.core.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "\n",
    "        # compute features\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
    "\n",
    "        # Add feature augmentation if specified.\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"fea_augment\"):\n",
    "            feats, fea_lens = self.hparams.fea_augment(feats, wav_lens)\n",
    "            tokens_bos = self.hparams.fea_augment.replicate_labels(tokens_bos)\n",
    "\n",
    "        # forward modules\n",
    "        src = self.modules.CNN(feats)\n",
    "\n",
    "        enc_out, pred = self.modules.Transformer(\n",
    "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index\n",
    "        )\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(enc_out)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "\n",
    "        # output layer for seq2seq log-probabilities\n",
    "        pred = self.modules.seq_lin(pred)\n",
    "        p_seq = self.hparams.log_softmax(pred)\n",
    "\n",
    "        # Compute outputs\n",
    "        hyps = None\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        is_valid_search = (\n",
    "            stage == sb.Stage.VALID\n",
    "            and current_epoch % self.hparams.valid_search_interval == 0\n",
    "        )\n",
    "        is_test_search = stage == sb.Stage.TEST\n",
    "\n",
    "        if any([is_valid_search, is_test_search]):\n",
    "            # Note: For valid_search, for the sake of efficiency, we only perform beamsearch with\n",
    "            # limited capacity and no LM to give user some idea of how the AM is doing\n",
    "\n",
    "            # Decide searcher for inference: valid or test search\n",
    "            if stage == sb.Stage.VALID:\n",
    "                hyps, _, _, _ = self.hparams.valid_search(\n",
    "                    enc_out.detach(), wav_lens\n",
    "                )\n",
    "            else:\n",
    "                hyps, _, _, _ = self.hparams.test_search(\n",
    "                    enc_out.detach(), wav_lens\n",
    "                )\n",
    "\n",
    "        return p_ctc, p_seq, wav_lens, hyps\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (CTC+NLL) given predictions and targets.\"\"\"\n",
    "\n",
    "        (p_ctc, p_seq, wav_lens, hyps) = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            # Labels must be extended if parallel augmentation or concatenated\n",
    "            # augmentation was performed on the input (increasing the time dimension)\n",
    "            if hasattr(self.hparams, \"fea_augment\"):\n",
    "                (\n",
    "                    tokens,\n",
    "                    tokens_lens,\n",
    "                    tokens_eos,\n",
    "                    tokens_eos_lens,\n",
    "                ) = self.hparams.fea_augment.replicate_multiple_labels(\n",
    "                    tokens, tokens_lens, tokens_eos, tokens_eos_lens\n",
    "                )\n",
    "\n",
    "        loss_seq = self.hparams.seq_cost(\n",
    "            p_seq, tokens_eos, length=tokens_eos_lens\n",
    "        ).sum()\n",
    "\n",
    "        loss_ctc = self.hparams.ctc_cost(\n",
    "            p_ctc, tokens, wav_lens, tokens_lens\n",
    "        ).sum()\n",
    "\n",
    "        loss = (\n",
    "            self.hparams.ctc_weight * loss_ctc\n",
    "            + (1 - self.hparams.ctc_weight) * loss_seq\n",
    "        )\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            current_epoch = self.hparams.epoch_counter.current\n",
    "            valid_search_interval = self.hparams.valid_search_interval\n",
    "            if current_epoch % valid_search_interval == 0 or (\n",
    "                stage == sb.Stage.TEST\n",
    "            ):\n",
    "                # Decode token terms to words\n",
    "                predicted_words = [\n",
    "                    tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
    "                ]\n",
    "                target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "                self.wer_metric.append(ids, predicted_words, target_words)\n",
    "\n",
    "            # compute the accuracy of the one-step-forward prediction\n",
    "            self.acc_metric.append(p_seq, tokens_eos, tokens_eos_lens)\n",
    "        return loss\n",
    "\n",
    "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
    "        \"\"\"perform checkpoint average if needed\"\"\"\n",
    "        super().on_evaluate_start()\n",
    "\n",
    "        ckpts = self.checkpointer.find_checkpoints(\n",
    "            max_key=max_key, min_key=min_key\n",
    "        )\n",
    "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
    "            ckpts, recoverable_name=\"model\"\n",
    "        )\n",
    "\n",
    "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
    "        self.hparams.model.eval()\n",
    "        print(\"Loaded the average\")\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.acc_metric = self.hparams.acc_computer()\n",
    "            self.wer_metric = self.hparams.error_rate_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        # Compute/store important stats\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"ACC\"] = self.acc_metric.summarize()\n",
    "            current_epoch = self.hparams.epoch_counter.current\n",
    "            valid_search_interval = self.hparams.valid_search_interval\n",
    "            if (\n",
    "                current_epoch % valid_search_interval == 0\n",
    "                or stage == sb.Stage.TEST\n",
    "            ):\n",
    "                stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        # log stats and save checkpoint at end-of-epoch\n",
    "        if stage == sb.Stage.VALID:\n",
    "            lr = self.hparams.noam_annealing.current_lr\n",
    "            steps = self.optimizer_step\n",
    "            optimizer = self.optimizer.__class__.__name__\n",
    "\n",
    "            epoch_stats = {\n",
    "                \"epoch\": epoch,\n",
    "                \"lr\": lr,\n",
    "                \"steps\": steps,\n",
    "                \"optimizer\": optimizer,\n",
    "            }\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta=epoch_stats,\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"ACC\": stage_stats[\"ACC\"], \"epoch\": epoch},\n",
    "                max_keys=[\"ACC\"],\n",
    "                num_to_keep=self.hparams.avg_checkpoints,\n",
    "            )\n",
    "\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stage_stats,\n",
    "            )\n",
    "            if if_main_process():\n",
    "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
    "                    self.wer_metric.write_stats(w)\n",
    "\n",
    "            # save the averaged checkpoint at the end of the evaluation stage\n",
    "            # delete the rest of the intermediate checkpoints\n",
    "            # ACC is set to 1.1 so checkpointer only keeps the averaged checkpoint\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"ACC\": 1.1, \"epoch\": epoch},\n",
    "                max_keys=[\"ACC\"],\n",
    "                num_to_keep=1,\n",
    "            )\n",
    "\n",
    "    def on_fit_batch_end(self, batch, outputs, loss, should_step):\n",
    "        \"\"\"At the end of the optimizer step, apply noam annealing.\"\"\"\n",
    "        if should_step:\n",
    "            self.hparams.noam_annealing(self.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88a094",
   "metadata": {},
   "source": [
    "### Finetuned Conformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0590045",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"/home/ubuntu/asr/hparam/conformer_large.yaml\"\n",
    "overrides = {\"data_folder\": \"/home/ubuntu/asr/datasets/LibriSpeech\"}\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79cbee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain = ASR(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"Adam\"],\n",
    "        hparams=hparams,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02b8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_brain.hparams.model.load_state_dict(torch.load(\"/home/ubuntu/asr/speechbrain/recipes/LibriSpeech/ASR/transformer/results/conformer_large/3407/save/CKPT+2024-04-19+19-19-29+00/model.ckpt\"), strict=True)\n",
    "\n",
    "finetuned_conformer = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee84aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported and loaded Weights of ConvolutionFrontEnd....\n",
      "Exported and loaded Weights of Transformer module....\n",
      "Exported and loaded Weights of linear layer related to seq_lin module....\n",
      "Exported and loaded Weights of linear layer related to ctc_lin module....\n",
      "Exported and loaded Weights of normalizer module....\n"
     ]
    }
   ],
   "source": [
    "torch.save(asr_brain.modules.CNN.state_dict(), 'weights_file_cnn_1') \n",
    "finetuned_conformer.mods.asr_model[0].load_state_dict(torch.load('weights_file_cnn_1'))\n",
    "print(\"Exported and loaded Weights of ConvolutionFrontEnd....\")\n",
    "\n",
    "torch.save(asr_brain.modules.Transformer.state_dict(), 'weights_file_transformer_1') #\n",
    "finetuned_conformer.mods.asr_model[1].load_state_dict(torch.load('weights_file_transformer_1'))\n",
    "print(\"Exported and loaded Weights of Transformer module....\")\n",
    "\n",
    "torch.save(asr_brain.modules.seq_lin.state_dict(), 'weights_file_seq_lin_1') #\n",
    "finetuned_conformer.mods.asr_model[2].load_state_dict(torch.load('weights_file_seq_lin_1'))\n",
    "print(\"Exported and loaded Weights of linear layer related to seq_lin module....\")\n",
    "\n",
    "torch.save(asr_brain.modules.ctc_lin.state_dict(), 'weights_file_ctc_lin_1') #\n",
    "finetuned_conformer.mods.asr_model[3].load_state_dict(torch.load('weights_file_ctc_lin_1'))\n",
    "print(\"Exported and loaded Weights of linear layer related to ctc_lin module....\")\n",
    "\n",
    "torch.save(asr_brain.modules.normalize.state_dict(), 'weights_file_normalizer_1') #\n",
    "finetuned_conformer.mods.normalizer.load_state_dict(torch.load('weights_file_normalizer_1'))\n",
    "print(\"Exported and loaded Weights of normalizer module....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58b309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fb56682",
   "metadata": {},
   "source": [
    "### Finetuned TSConformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54dd0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams_file = \"/home/ubuntu/asr/hparam/conformer_large.yaml\"\n",
    "overrides = {\"data_folder\": \"/home/ubuntu/asr/datasets/LibriSpeech\"}\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "asr_brain = ASR(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"Adam\"],\n",
    "        hparams=hparams,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "finetuned_tsconformer = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(12):\n",
    "    asr_brain.modules.Transformer.encoder.layers[i].mha_layer = LinearAttn(dim = 512, dim_head = 16, heads = 8)\n",
    "    finetuned_tsconformer.mods.transformer.encoder.layers[i].mha_layer = LinearAttn(dim = 512, dim_head = 16, heads = 8)\n",
    "\n",
    "\n",
    "asr_brain.hparams.model.load_state_dict(torch.load(\"/home/ubuntu/asr/speechbrain/recipes/LibriSpeech/ASR/transformer/results/tsconformer_large/3407/save/CKPT+2024-04-24+11-40-51+00/model.ckpt\"), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "767d1334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported and loaded Weights of ConvolutionFrontEnd....\n",
      "Exported and loaded Weights of Transformer module....\n",
      "Exported and loaded Weights of linear layer related to seq_lin module....\n",
      "Exported and loaded Weights of linear layer related to ctc_lin module....\n",
      "Exported and loaded Weights of normalizer module....\n"
     ]
    }
   ],
   "source": [
    "torch.save(asr_brain.modules.CNN.state_dict(), 'weights_file_cnn_2') \n",
    "finetuned_tsconformer.mods.asr_model[0].load_state_dict(torch.load('weights_file_cnn_2'))\n",
    "print(\"Exported and loaded Weights of ConvolutionFrontEnd....\")\n",
    "\n",
    "torch.save(asr_brain.modules.Transformer.state_dict(), 'weights_file_transformer_2') #\n",
    "finetuned_tsconformer.mods.asr_model[1].load_state_dict(torch.load('weights_file_transformer_2'))\n",
    "print(\"Exported and loaded Weights of Transformer module....\")\n",
    "\n",
    "torch.save(asr_brain.modules.seq_lin.state_dict(), 'weights_file_seq_lin_2') #\n",
    "finetuned_tsconformer.mods.asr_model[2].load_state_dict(torch.load('weights_file_seq_lin_2'))\n",
    "print(\"Exported and loaded Weights of linear layer related to seq_lin module....\")\n",
    "\n",
    "torch.save(asr_brain.modules.ctc_lin.state_dict(), 'weights_file_ctc_lin_2') #\n",
    "finetuned_tsconformer.mods.asr_model[3].load_state_dict(torch.load('weights_file_ctc_lin_2'))\n",
    "print(\"Exported and loaded Weights of linear layer related to ctc_lin module....\")\n",
    "\n",
    "torch.save(asr_brain.modules.normalize.state_dict(), 'weights_file_normalizer_2') #\n",
    "finetuned_tsconformer.mods.normalizer.load_state_dict(torch.load('weights_file_normalizer_2'))\n",
    "print(\"Exported and loaded Weights of normalizer module....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6446f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d699d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3fd64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file, cpretrained=pretrained_conformer, cfinetuned=finetuned_conformer, tscfinetuned=finetuned_tsconformer):\n",
    "    cpretrained_text = cpretrained.transcribe_file(audio_file)\n",
    "    cfinetuned_test = cfinetuned.transcribe_file(audio_file)\n",
    "    tscfinetuned_text = tscfinetuned.transcribe_file(audio_file)\n",
    "    return audio_file, cpretrained_text, cfinetuned_test, tscfinetuned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63fd818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/asr/datasets/LibriSpeech/test-clean/61/70970/61-70970-0000.flac\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"YOUNG FITZOOTH HAD BEEN COMMANDED TO HIS MOTHER'S CHAMBER SO SOON AS HE HAD COME OUT FROM HIS CONVERSE WITH THE SQUIRE\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Input Audio File : /home/ubuntu/asr/datasets/LibriSpeech/test-clean/908/157963/908-157963-0001.flac\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8dccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/gradio/analytics.py:99: UserWarning: unable to parse version details from package URL.\n",
      "  warnings.warn(\"unable to parse version details from package URL.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    title=\"ASR for Noisy Audio!\",\n",
    "    fn=transcribe_audio,\n",
    "    inputs = [\"text\"],\n",
    "    outputs=[gr.Audio(label=\"Audio Transcript\"), gr.Textbox(label=\"Pretrained Conformer\", lines=3), \n",
    "             gr.Textbox(label=\"Finetuned Conformer\", lines=3), gr.Textbox(label=\"Finetuned TSConformer\", lines=3)],\n",
    ")\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de51ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
