Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.
/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Mean CER on wham-noisy test-clean of LibriSpeech for wav2vec2 model : 0.08833001


Mean WER on wham-noisy test-clean of LibriSpeech for wav2vec2 model : 0.101971805


Mean CER on wham-noisy test-clean of LibriSpeech for conformer model : 0.09611323


Mean WER on wham-noisy test-clean of LibriSpeech for conformer model : 0.10790158
